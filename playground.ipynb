{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aa572a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "newest\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Stable imports (won't reload)\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "# Force reload of project modules\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "modules_to_reload = [\n",
    "    'calibration', 'compress_qk', 'compress_mlp', 'compress_vo',\n",
    "    'compression_utils', 'eval', 'model_utils', 'patchers.patch'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "from calibration import get_model_attrs, load_calibs\n",
    "from compress_qk import compress_qk, compress_qk_svd\n",
    "from compress_mlp import compress_mlp\n",
    "from compress_vo import compress_vo\n",
    "from compression_utils import allocate_global_sparsity\n",
    "from eval import compute_perplexity, load_calibration_texts, load_eval_texts\n",
    "from model_utils import load_model, reload_compressed_model, save_compressed_model, save_model\n",
    "from patchers.patch import patch_config\n",
    "\n",
    "logger = logging.getLogger(\"MoDeGPT\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    console = logging.StreamHandler()\n",
    "    console.setFormatter(formatter)\n",
    "    logger.addHandler(console)\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    file = logging.FileHandler(\"logs/run_modegpt.log\")\n",
    "    file.setFormatter(formatter)\n",
    "    logger.addHandler(file)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model: str = \"meta-llama/Llama-2-7b-hf\"\n",
    "    compression_ratio: float = 0.4\n",
    "    calib_size: str = \"16\"\n",
    "    eval_size: str = \"8\"\n",
    "    output_dir: str = \"./compressed_output/llama2-7b\"\n",
    "    device: int = 0\n",
    "    skip: str = \"mlp\"\n",
    "    local_model_path: str = \"\"\n",
    "    load_calibs_from: str = \"./calibs/llama2-7b_sz16.pt\"\n",
    "    calibs_save_path: str = \"\"\n",
    "    calibs_batch_size: int = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5851a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    compression_ratio=0.4,\n",
    "    calib_size=\"16\",\n",
    "    eval_size=\"8\",\n",
    "    output_dir=\"./compressed_output/llama2-7b\",\n",
    "    device=0,\n",
    "    skip=\"mlp\",\n",
    "    load_calibs_from=\"\",\n",
    "    calibs_save_path=\"./calibs/llama2-7b_qk-updated_sz16.pt\",\n",
    "    calibs_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f501825",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7efb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:07:45,803 - INFO - Loading model from: meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s]\n",
      "2025-11-09 20:08:15,732 - INFO - ✔ Loaded model on cuda:0 with float16.\n",
      "2025-11-09 20:08:15,734 - INFO - No pad_token found. Set pad_token = eos_token.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, config = load_model(args.model, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b1295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_texts, eval_texts, cov_mlp, cov_q, cov_k, cov_x, bi_scores, layer_keep_ratios = (\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2102e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8528468",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compressed, tokenizer_compressed = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e50a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader():\n",
    "    global calib_texts, eval_texts, cov_mlp, cov_q, cov_k, cov_x, bi_scores, layer_keep_ratios\n",
    "    logger.info(\"Loading calibration and evaluation texts...\")\n",
    "    calib_texts = load_calibration_texts(\n",
    "        args.calib_size, model, tokenizer, batch_size=int(args.calibs_batch_size)\n",
    "    )\n",
    "    eval_texts = load_eval_texts(\n",
    "        args.eval_size, model, tokenizer, batch_size=args.calibs_batch_size\n",
    "    )\n",
    "\n",
    "    cov_mlp, cov_q, cov_k, cov_x, bi_scores = load_calibs(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        calib_texts,\n",
    "        int(args.calibs_batch_size),\n",
    "        load_calibs_from=args.load_calibs_from,\n",
    "        calibs_save_path=args.calibs_save_path,\n",
    "    )\n",
    "\n",
    "    layer_keep_ratios = allocate_global_sparsity(\n",
    "        bi_scores, compression_ratio=args.compression_ratio\n",
    "    )\n",
    "\n",
    "\n",
    "def compress():\n",
    "    global rotary_mask\n",
    "\n",
    "    slice_dims = True\n",
    "    ridge_lambda = 1e-2\n",
    "\n",
    "    # compress_mlp(\n",
    "    #     model=model,\n",
    "    #     cov=cov_mlp,\n",
    "    #     keep_ratios=layer_keep_ratios,\n",
    "    #     ridge_lambda=1e-3,\n",
    "    #     slice_dims=True,\n",
    "    # )\n",
    "\n",
    "    logger.info(\"Compress QK\")\n",
    "    rotary_mask = compress_qk(\n",
    "        model=model,\n",
    "        cov=(cov_q, cov_k),\n",
    "        keep_ratios=layer_keep_ratios,\n",
    "        ridge_lambda=ridge_lambda,\n",
    "        slice_dims=slice_dims,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Compress VO\")\n",
    "    compress_vo(\n",
    "        model=model,\n",
    "        cov=cov_x,\n",
    "        keep_ratios=layer_keep_ratios,\n",
    "        ridge_lambda=ridge_lambda,\n",
    "        slice_dims=slice_dims,\n",
    "    )\n",
    "\n",
    "\n",
    "def reload_save_comp():\n",
    "    global model, model_compressed, tokenizer_compressed\n",
    "\n",
    "    og_config = patch_config(model)\n",
    "\n",
    "    rebuild_path = \"./patchers/LlamaRebuild.py\"\n",
    "\n",
    "    if model_compressed is not None:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    save_compressed_model(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        rotary_masks=rotary_mask,\n",
    "        rebuild_path=rebuild_path,\n",
    "        save_dir=args.output_dir,\n",
    "        source_model_name=args.model,\n",
    "    )\n",
    "\n",
    "    model.config = og_config\n",
    "\n",
    "    model_compressed, tokenizer_compressed = reload_compressed_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4471c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:47:27,859 - INFO - Loading calibration and evaluation texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:47:42,925 - INFO - Calibrating model...\n",
      "2025-11-09 20:47:42,926 - INFO - Calibrating model\n",
      "2025-11-09 20:47:42,926 - INFO - n_layers=32, n_heads=32, d_model=4096, head_dim=128\n",
      "2025-11-09 20:47:42,927 - INFO - Detected architecture: llama\n",
      "2025-11-09 20:47:42,928 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:42,928 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:42,977 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:42,977 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,018 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,018 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,059 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,059 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,100 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,101 - INFO - n_inner = 11008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wonder if i'll see this\n",
      "hello from __calibrate_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 20:47:43,141 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,142 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,183 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,183 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,224 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,225 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,268 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,269 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,309 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,310 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,351 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,352 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,392 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,393 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,433 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,434 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,474 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,475 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,515 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,516 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,593 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,594 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,635 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,636 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,678 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,679 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,723 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,724 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,779 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,779 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,849 - INFO - n_inner = 11008\n",
      "2025-11-09 20:47:43,850 - INFO - n_inner = 11008\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "loader()\n",
    "compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9aeada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:43:56,892 - INFO - n_layers=32, n_heads=32, d_model=4096, head_dim=128\n",
      "2025-11-09 19:44:18,980 - INFO - ✔ Model, tokenizer, and tokenizer_source.txt saved to ./compressed_output/llama2-7b\n",
      "2025-11-09 19:44:18,982 - INFO - Reloading compressed model from: ./compressed_output/llama2-7b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_path = /blue/sgao1/cc22bc.fsu/prog/MoDeGPT/compressed_output/llama2-7b/rotary_masks.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.44it/s]\n",
      "2025-11-09 19:44:29,413 - INFO - ✔ Reloaded compressed model to cuda:0 successfully.\n"
     ]
    }
   ],
   "source": [
    "reload_save_comp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d15e3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "inputs['input_ids'].shape = torch.Size([8, 2048])\n",
      "q.shape = torch.Size([8, 32, 2048, 110]), k.shape = torch.Size([8, 32, 2048, 110])\n",
      "rotary_mask.numel() = 3520\n",
      "tensor([[[[ 47,  48,  41,  ...,  71,  87,  81]],\n",
      "\n",
      "         [[ 31,  40,  22,  ...,  93,  66, 100]],\n",
      "\n",
      "         [[ 47,  61,  63,  ...,  86,  84,  73]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 52,  58,  57,  ...,  95,  74,  75]],\n",
      "\n",
      "         [[ 19,   2,  12,  ..., 115, 111, 114]],\n",
      "\n",
      "         [[  2,   0,   5,  ...,  70,  90, 116]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 110\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 110])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 110])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 110])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 110])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 110]), k_embed.shape = torch.Size([8, 32, 2048, 110])\n",
      "q.shape = torch.Size([8, 32, 2048, 100]), k.shape = torch.Size([8, 32, 2048, 100])\n",
      "rotary_mask.numel() = 3200\n",
      "tensor([[[[ 57,  55,  61,  ...,  91, 108,  62]],\n",
      "\n",
      "         [[ 36,  31,  34,  ...,  68,  64,  66]],\n",
      "\n",
      "         [[ 29,  24,  52,  ...,  50,  94,  96]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 20,  10,  21,  ...,  50,  51,  98]],\n",
      "\n",
      "         [[ 53,  54,  29,  ...,  68,  95,  93]],\n",
      "\n",
      "         [[ 63,  62,  15,  ...,  96,  59,  86]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 100\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 100])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 100])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 100])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 100])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 100]), k_embed.shape = torch.Size([8, 32, 2048, 100])\n",
      "q.shape = torch.Size([8, 32, 2048, 84]), k.shape = torch.Size([8, 32, 2048, 84])\n",
      "rotary_mask.numel() = 2688\n",
      "tensor([[[[ 52,  51,  59,  ...,  83,  42,  69]],\n",
      "\n",
      "         [[ 54,  55,  58,  ...,  81,  89,  50]],\n",
      "\n",
      "         [[ 56,  50,  57,  ...,  69,  87,  78]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 53,  51,  52,  ..., 104,  79,  90]],\n",
      "\n",
      "         [[ 52,  51,  15,  ...,  98,  84, 104]],\n",
      "\n",
      "         [[ 52,  56,  62,  ..., 105,  99,  85]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 84\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 84])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 84])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 84])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 84])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 84]), k_embed.shape = torch.Size([8, 32, 2048, 84])\n",
      "q.shape = torch.Size([8, 32, 2048, 86]), k.shape = torch.Size([8, 32, 2048, 86])\n",
      "rotary_mask.numel() = 2752\n",
      "tensor([[[[ 61,  63,  52,  ...,  47,  91,  82]],\n",
      "\n",
      "         [[ 53,  50,  24,  ..., 100,  80,  85]],\n",
      "\n",
      "         [[ 61,  51,  20,  ...,  89,  75,  95]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 61,  50,  43,  ...,  87,  64,  79]],\n",
      "\n",
      "         [[ 59,  60,  50,  ...,  80,  75,  89]],\n",
      "\n",
      "         [[ 62,  61,  60,  ...,  76,  63,  64]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 86\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 86])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 86])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 86])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 86])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 86]), k_embed.shape = torch.Size([8, 32, 2048, 86])\n",
      "q.shape = torch.Size([8, 32, 2048, 86]), k.shape = torch.Size([8, 32, 2048, 86])\n",
      "rotary_mask.numel() = 2752\n",
      "tensor([[[[ 51,  60,  57,  ...,  92,  79,  72]],\n",
      "\n",
      "         [[ 54,  51,  58,  ...,  74,  63,  67]],\n",
      "\n",
      "         [[ 58,  60,  52,  ...,  90, 104, 105]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 53,  51,  56,  ...,  71,  64,  57]],\n",
      "\n",
      "         [[ 51,  59,  58,  ...,  75,  74,  77]],\n",
      "\n",
      "         [[ 52,  44,  29,  ...,  75,  79,  85]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 86\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 86])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 86])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 86])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 86])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 86]), k_embed.shape = torch.Size([8, 32, 2048, 86])\n",
      "q.shape = torch.Size([8, 32, 2048, 86]), k.shape = torch.Size([8, 32, 2048, 86])\n",
      "rotary_mask.numel() = 2752\n",
      "tensor([[[[ 51,  15,  24,  ...,  90, 106,  87]],\n",
      "\n",
      "         [[ 56,  57,  50,  ...,  71,  83,  70]],\n",
      "\n",
      "         [[ 63,  60,  56,  ...,  65,  96,  64]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 52,  61,  59,  ...,  74,  73,  84]],\n",
      "\n",
      "         [[ 59,  44,  50,  ...,  99,  73,  85]],\n",
      "\n",
      "         [[ 54,  63,  61,  ...,  91,  53,  60]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 86\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 86])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 86])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 86])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 86])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 86]), k_embed.shape = torch.Size([8, 32, 2048, 86])\n",
      "q.shape = torch.Size([8, 32, 2048, 84]), k.shape = torch.Size([8, 32, 2048, 84])\n",
      "rotary_mask.numel() = 2688\n",
      "tensor([[[[53, 58, 56,  ..., 84, 75, 63]],\n",
      "\n",
      "         [[57, 60, 49,  ..., 87, 74, 63]],\n",
      "\n",
      "         [[54, 55, 48,  ..., 81, 73, 64]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[51, 44, 29,  ..., 98, 64, 70]],\n",
      "\n",
      "         [[58, 54, 57,  ..., 67, 72, 62]],\n",
      "\n",
      "         [[57, 50, 61,  ..., 79, 69, 65]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 84\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 84])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 84])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 84])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 84])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 84]), k_embed.shape = torch.Size([8, 32, 2048, 84])\n",
      "q.shape = torch.Size([8, 32, 2048, 84]), k.shape = torch.Size([8, 32, 2048, 84])\n",
      "rotary_mask.numel() = 2688\n",
      "tensor([[[[ 60,  55,  50,  ...,  70,  73,  72]],\n",
      "\n",
      "         [[ 52,  57,  25,  ...,  88,  75,  74]],\n",
      "\n",
      "         [[ 53,  51,  45,  ...,  70,  79,  67]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 57,  33,  29,  ...,  78,  69,  58]],\n",
      "\n",
      "         [[ 55,  61,  63,  ...,  91, 101,  90]],\n",
      "\n",
      "         [[ 61,  60,  58,  ...,  73,  79,  61]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 84\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 84])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 84])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 84])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 84])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 84]), k_embed.shape = torch.Size([8, 32, 2048, 84])\n",
      "q.shape = torch.Size([8, 32, 2048, 84]), k.shape = torch.Size([8, 32, 2048, 84])\n",
      "rotary_mask.numel() = 2688\n",
      "tensor([[[[54, 63, 59,  ..., 52, 65, 79]],\n",
      "\n",
      "         [[54, 48, 29,  ..., 88, 74, 82]],\n",
      "\n",
      "         [[51, 24, 43,  ..., 65, 70, 82]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[59, 29, 24,  ..., 85, 83, 97]],\n",
      "\n",
      "         [[51, 45, 24,  ..., 92, 88, 82]],\n",
      "\n",
      "         [[61, 63, 51,  ..., 66, 64, 68]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 84\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 84])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 84])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 84])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 84])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 84]), k_embed.shape = torch.Size([8, 32, 2048, 84])\n",
      "q.shape = torch.Size([8, 32, 2048, 82]), k.shape = torch.Size([8, 32, 2048, 82])\n",
      "rotary_mask.numel() = 2624\n",
      "tensor([[[[63, 55, 49,  ..., 86, 51, 89]],\n",
      "\n",
      "         [[53, 60, 63,  ..., 96, 63, 89]],\n",
      "\n",
      "         [[51, 44, 34,  ..., 58, 79, 86]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[63, 62, 51,  ..., 74, 63, 78]],\n",
      "\n",
      "         [[60, 59, 53,  ..., 57, 78, 77]],\n",
      "\n",
      "         [[53, 36, 29,  ..., 67, 75, 66]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 82\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 82])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 82])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 82])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 82])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 82]), k_embed.shape = torch.Size([8, 32, 2048, 82])\n",
      "q.shape = torch.Size([8, 32, 2048, 80]), k.shape = torch.Size([8, 32, 2048, 80])\n",
      "rotary_mask.numel() = 2560\n",
      "tensor([[[[56, 61, 46,  ..., 79, 68, 58]],\n",
      "\n",
      "         [[54,  0, 58,  ..., 84, 81, 83]],\n",
      "\n",
      "         [[53, 63, 61,  ..., 72, 80, 69]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[62, 58, 63,  ..., 74, 67, 66]],\n",
      "\n",
      "         [[60, 50, 62,  ..., 69, 57, 76]],\n",
      "\n",
      "         [[55, 51, 32,  ..., 81, 84, 68]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 80\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 80])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 80])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 80])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 80])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 80]), k_embed.shape = torch.Size([8, 32, 2048, 80])\n",
      "q.shape = torch.Size([8, 32, 2048, 78]), k.shape = torch.Size([8, 32, 2048, 78])\n",
      "rotary_mask.numel() = 2496\n",
      "tensor([[[[54, 31, 48,  ..., 73, 85, 83]],\n",
      "\n",
      "         [[53, 49, 60,  ..., 75, 57, 66]],\n",
      "\n",
      "         [[63, 62, 61,  ..., 66, 63, 65]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[55, 57, 33,  ..., 75, 70, 62]],\n",
      "\n",
      "         [[62, 58, 57,  ..., 58, 62, 71]],\n",
      "\n",
      "         [[56, 42, 62,  ..., 70, 60, 58]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 78\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 78])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 78])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 78])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 78])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 78]), k_embed.shape = torch.Size([8, 32, 2048, 78])\n",
      "q.shape = torch.Size([8, 32, 2048, 78]), k.shape = torch.Size([8, 32, 2048, 78])\n",
      "rotary_mask.numel() = 2496\n",
      "tensor([[[[56, 50, 24,  ..., 70, 69, 73]],\n",
      "\n",
      "         [[52, 36, 29,  ..., 61, 79, 72]],\n",
      "\n",
      "         [[62, 61, 63,  ..., 63, 64, 65]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[52, 32, 62,  ..., 75, 93, 97]],\n",
      "\n",
      "         [[63, 51, 43,  ..., 65, 76, 72]],\n",
      "\n",
      "         [[44, 50, 29,  ..., 87, 74, 88]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 78\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 78])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 78])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 78])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 78])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 78]), k_embed.shape = torch.Size([8, 32, 2048, 78])\n",
      "q.shape = torch.Size([8, 32, 2048, 78]), k.shape = torch.Size([8, 32, 2048, 78])\n",
      "rotary_mask.numel() = 2496\n",
      "tensor([[[[56, 34, 24,  ..., 90, 78, 81]],\n",
      "\n",
      "         [[52, 44, 34,  ..., 68, 81, 72]],\n",
      "\n",
      "         [[54, 62, 51,  ..., 73, 64, 77]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[52, 24, 15,  ..., 84, 79, 97]],\n",
      "\n",
      "         [[55, 52, 62,  ..., 60, 61, 73]],\n",
      "\n",
      "         [[57, 63, 49,  ..., 75, 74, 63]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 78\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 78])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 78])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 78])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 78])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 78]), k_embed.shape = torch.Size([8, 32, 2048, 78])\n",
      "q.shape = torch.Size([8, 32, 2048, 76]), k.shape = torch.Size([8, 32, 2048, 76])\n",
      "rotary_mask.numel() = 2432\n",
      "tensor([[[[ 55,  24,  31,  ...,  70,  64,  44]],\n",
      "\n",
      "         [[ 61,  63,  51,  ...,  57,  75,  72]],\n",
      "\n",
      "         [[ 54,  41,  60,  ...,  76,  64,  72]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 62,  61,  50,  ...,  66,  62,  59]],\n",
      "\n",
      "         [[ 58,  50,  29,  ...,  61,  65,  94]],\n",
      "\n",
      "         [[ 52,  59,  24,  ...,  85, 100,  61]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 76\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 76])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 76])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 76])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 76])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 76]), k_embed.shape = torch.Size([8, 32, 2048, 76])\n",
      "q.shape = torch.Size([8, 32, 2048, 76]), k.shape = torch.Size([8, 32, 2048, 76])\n",
      "rotary_mask.numel() = 2432\n",
      "tensor([[[[54, 63, 30,  ..., 53, 84, 79]],\n",
      "\n",
      "         [[52, 63, 24,  ..., 72, 75, 87]],\n",
      "\n",
      "         [[51, 43, 31,  ..., 76, 98, 87]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[62, 52, 57,  ..., 61, 75, 80]],\n",
      "\n",
      "         [[63, 47, 42,  ..., 75, 81, 84]],\n",
      "\n",
      "         [[53, 44, 62,  ..., 72, 55, 57]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 76\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 76])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 76])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 76])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 76])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 76]), k_embed.shape = torch.Size([8, 32, 2048, 76])\n",
      "q.shape = torch.Size([8, 32, 2048, 76]), k.shape = torch.Size([8, 32, 2048, 76])\n",
      "rotary_mask.numel() = 2432\n",
      "tensor([[[[55, 39, 25,  ..., 64, 68, 53]],\n",
      "\n",
      "         [[62, 59, 63,  ..., 69, 57, 71]],\n",
      "\n",
      "         [[54, 42, 29,  ..., 73, 76, 83]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[52, 59, 43,  ..., 68, 70, 72]],\n",
      "\n",
      "         [[63, 59, 62,  ..., 69, 67, 62]],\n",
      "\n",
      "         [[57, 50, 63,  ..., 68, 76, 74]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 76\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 76])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 76])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 76])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 76])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 76]), k_embed.shape = torch.Size([8, 32, 2048, 76])\n",
      "q.shape = torch.Size([8, 32, 2048, 72]), k.shape = torch.Size([8, 32, 2048, 72])\n",
      "rotary_mask.numel() = 2304\n",
      "tensor([[[[53, 60, 61,  ..., 78, 72, 61]],\n",
      "\n",
      "         [[53, 55, 35,  ..., 77, 74, 64]],\n",
      "\n",
      "         [[55, 60, 53,  ..., 71, 78, 73]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[59, 23, 20,  ..., 77, 41, 72]],\n",
      "\n",
      "         [[56, 55, 45,  ..., 79, 75, 77]],\n",
      "\n",
      "         [[56, 62, 63,  ..., 73, 57, 75]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 72\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 72])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 72])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 72])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 72])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 72]), k_embed.shape = torch.Size([8, 32, 2048, 72])\n",
      "q.shape = torch.Size([8, 32, 2048, 70]), k.shape = torch.Size([8, 32, 2048, 70])\n",
      "rotary_mask.numel() = 2240\n",
      "tensor([[[[58, 30, 24,  ..., 81, 80, 85]],\n",
      "\n",
      "         [[61, 52, 44,  ..., 62, 66, 70]],\n",
      "\n",
      "         [[56, 24, 30,  ..., 66, 73, 61]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[56, 28, 25,  ..., 79, 82, 52]],\n",
      "\n",
      "         [[62, 51, 44,  ..., 68, 71, 63]],\n",
      "\n",
      "         [[60, 48, 58,  ..., 78, 60, 61]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 70\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 70])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 70])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 70])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 70])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 70]), k_embed.shape = torch.Size([8, 32, 2048, 70])\n",
      "q.shape = torch.Size([8, 32, 2048, 68]), k.shape = torch.Size([8, 32, 2048, 68])\n",
      "rotary_mask.numel() = 2176\n",
      "tensor([[[[52, 44, 34,  ..., 87, 76, 84]],\n",
      "\n",
      "         [[54, 34, 30,  ..., 76, 97, 72]],\n",
      "\n",
      "         [[53, 34, 15,  ..., 70, 75, 74]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[55, 58, 24,  ..., 72, 65, 70]],\n",
      "\n",
      "         [[56, 44, 33,  ..., 65, 82, 79]],\n",
      "\n",
      "         [[57, 48, 43,  ..., 44, 62, 80]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 68\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 68])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 68])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 68])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 68])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 68]), k_embed.shape = torch.Size([8, 32, 2048, 68])\n",
      "q.shape = torch.Size([8, 32, 2048, 68]), k.shape = torch.Size([8, 32, 2048, 68])\n",
      "rotary_mask.numel() = 2176\n",
      "tensor([[[[61, 60, 58,  ..., 69, 61, 71]],\n",
      "\n",
      "         [[58, 62, 50,  ..., 75, 74, 59]],\n",
      "\n",
      "         [[53, 63, 30,  ..., 55, 86, 62]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[63, 61, 62,  ..., 62, 65, 68]],\n",
      "\n",
      "         [[60, 52, 63,  ..., 75, 74, 58]],\n",
      "\n",
      "         [[51, 60, 59,  ..., 69, 56, 66]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 68\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 68])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 68])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 68])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 68])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 68]), k_embed.shape = torch.Size([8, 32, 2048, 68])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:45:24,400 - INFO - Compressed model perplexity on WikiText2: 3620.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.shape = torch.Size([8, 32, 2048, 64]), k.shape = torch.Size([8, 32, 2048, 64])\n",
      "rotary_mask.numel() = 2048\n",
      "tensor([[[[57, 49, 36,  ..., 71, 72, 74]],\n",
      "\n",
      "         [[60, 62, 59,  ..., 75, 76, 57]],\n",
      "\n",
      "         [[60, 52, 59,  ..., 82, 77, 66]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[60, 61, 49,  ..., 77, 72, 51]],\n",
      "\n",
      "         [[61, 60, 46,  ..., 73, 69, 62]],\n",
      "\n",
      "         [[56, 63, 50,  ..., 79, 75, 57]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 64\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 64])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 64])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 64])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 64])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 64]), k_embed.shape = torch.Size([8, 32, 2048, 64])\n",
      "q.shape = torch.Size([8, 32, 2048, 64]), k.shape = torch.Size([8, 32, 2048, 64])\n",
      "rotary_mask.numel() = 2048\n",
      "tensor([[[[53, 63, 29,  ..., 84, 89, 64]],\n",
      "\n",
      "         [[60, 50, 29,  ..., 55, 74, 57]],\n",
      "\n",
      "         [[53, 62, 20,  ..., 42, 63, 90]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[58, 35, 31,  ..., 66, 49, 87]],\n",
      "\n",
      "         [[61, 58, 50,  ..., 72, 69, 67]],\n",
      "\n",
      "         [[61, 52, 49,  ..., 63, 69, 74]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 64\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 64])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 64])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 64])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 64])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 64]), k_embed.shape = torch.Size([8, 32, 2048, 64])\n",
      "q.shape = torch.Size([8, 32, 2048, 62]), k.shape = torch.Size([8, 32, 2048, 62])\n",
      "rotary_mask.numel() = 1984\n",
      "tensor([[[[55, 63, 59,  ..., 77, 73, 65]],\n",
      "\n",
      "         [[58, 24, 15,  ..., 82, 86, 59]],\n",
      "\n",
      "         [[58, 37, 28,  ..., 60, 72, 73]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[59, 62, 51,  ..., 61, 72, 67]],\n",
      "\n",
      "         [[57, 39, 29,  ..., 80, 72, 77]],\n",
      "\n",
      "         [[55, 62, 63,  ..., 50, 59, 61]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 62\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 62])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 62])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 62])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 62])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 62]), k_embed.shape = torch.Size([8, 32, 2048, 62])\n",
      "q.shape = torch.Size([8, 32, 2048, 60]), k.shape = torch.Size([8, 32, 2048, 60])\n",
      "rotary_mask.numel() = 1920\n",
      "tensor([[[[57, 31, 20,  ..., 52, 76, 72]],\n",
      "\n",
      "         [[59, 50, 58,  ..., 63, 78, 77]],\n",
      "\n",
      "         [[57, 30, 24,  ..., 83, 77, 70]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[61, 63, 59,  ..., 65, 60, 73]],\n",
      "\n",
      "         [[54, 63, 58,  ..., 66, 68, 55]],\n",
      "\n",
      "         [[55, 58, 24,  ..., 74, 75, 78]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 60\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 60])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 60])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 60])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 60])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 60]), k_embed.shape = torch.Size([8, 32, 2048, 60])\n",
      "q.shape = torch.Size([8, 32, 2048, 60]), k.shape = torch.Size([8, 32, 2048, 60])\n",
      "rotary_mask.numel() = 1920\n",
      "tensor([[[[63, 48, 60,  ..., 76, 73, 45]],\n",
      "\n",
      "         [[56, 31, 24,  ..., 76, 85, 75]],\n",
      "\n",
      "         [[59, 29, 15,  ..., 73, 74, 86]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[59, 20, 35,  ..., 64, 92, 79]],\n",
      "\n",
      "         [[57, 15, 28,  ..., 81, 78, 72]],\n",
      "\n",
      "         [[55, 58, 62,  ..., 49, 50, 72]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 60\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 60])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 60])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 60])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 60])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 60]), k_embed.shape = torch.Size([8, 32, 2048, 60])\n",
      "q.shape = torch.Size([8, 32, 2048, 60]), k.shape = torch.Size([8, 32, 2048, 60])\n",
      "rotary_mask.numel() = 1920\n",
      "tensor([[[[59, 37, 29,  ..., 82, 72, 79]],\n",
      "\n",
      "         [[59, 58, 43,  ..., 78, 74, 79]],\n",
      "\n",
      "         [[54, 36, 29,  ..., 60, 71, 77]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[55, 50, 58,  ..., 67, 49, 68]],\n",
      "\n",
      "         [[61, 34, 48,  ..., 75, 87, 70]],\n",
      "\n",
      "         [[61, 40, 32,  ..., 73, 74, 55]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 60\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 60])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 60])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 60])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 60])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 60]), k_embed.shape = torch.Size([8, 32, 2048, 60])\n",
      "q.shape = torch.Size([8, 32, 2048, 60]), k.shape = torch.Size([8, 32, 2048, 60])\n",
      "rotary_mask.numel() = 1920\n",
      "tensor([[[[55, 26, 15,  ..., 93, 67, 62]],\n",
      "\n",
      "         [[58, 34, 39,  ..., 63, 70, 60]],\n",
      "\n",
      "         [[52, 34, 25,  ..., 71, 84, 85]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[59, 31, 39,  ..., 72, 80, 51]],\n",
      "\n",
      "         [[55, 38, 19,  ..., 73, 59, 76]],\n",
      "\n",
      "         [[63, 60, 48,  ..., 60, 63, 44]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 60\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 60])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 60])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 60])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 60])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 60]), k_embed.shape = torch.Size([8, 32, 2048, 60])\n",
      "q.shape = torch.Size([8, 32, 2048, 60]), k.shape = torch.Size([8, 32, 2048, 60])\n",
      "rotary_mask.numel() = 1920\n",
      "tensor([[[[57, 51, 59,  ..., 78, 67, 88]],\n",
      "\n",
      "         [[53, 58, 24,  ..., 75, 80, 81]],\n",
      "\n",
      "         [[60, 20, 24,  ..., 72, 78, 65]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[63, 61, 48,  ..., 83, 67, 53]],\n",
      "\n",
      "         [[59, 19, 49,  ..., 84, 54, 67]],\n",
      "\n",
      "         [[55, 60, 58,  ..., 65, 49, 68]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 60\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 60])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 60])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 60])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 60])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 60]), k_embed.shape = torch.Size([8, 32, 2048, 60])\n",
      "q.shape = torch.Size([8, 32, 2048, 60]), k.shape = torch.Size([8, 32, 2048, 60])\n",
      "rotary_mask.numel() = 1920\n",
      "tensor([[[[53, 29, 44,  ..., 40, 93, 89]],\n",
      "\n",
      "         [[53, 61, 57,  ..., 82, 61, 57]],\n",
      "\n",
      "         [[62, 58, 63,  ..., 60, 68, 61]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[55, 31, 15,  ..., 59, 62, 64]],\n",
      "\n",
      "         [[61, 48, 34,  ..., 86, 82, 88]],\n",
      "\n",
      "         [[63, 60, 49,  ..., 76, 75, 74]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 60\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 60])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 60])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 60])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 60])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 60]), k_embed.shape = torch.Size([8, 32, 2048, 60])\n",
      "q.shape = torch.Size([8, 32, 2048, 66]), k.shape = torch.Size([8, 32, 2048, 66])\n",
      "rotary_mask.numel() = 2112\n",
      "tensor([[[[54, 29, 25,  ..., 94, 63, 71]],\n",
      "\n",
      "         [[57, 50, 29,  ..., 73, 75, 84]],\n",
      "\n",
      "         [[54, 31, 25,  ..., 88, 84, 81]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[55, 34, 50,  ..., 75, 77, 81]],\n",
      "\n",
      "         [[57, 31, 15,  ..., 88, 81, 80]],\n",
      "\n",
      "         [[55, 33, 40,  ..., 89, 74, 72]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 66\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 66])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 66])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 66])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 66])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 66]), k_embed.shape = torch.Size([8, 32, 2048, 66])\n",
      "q.shape = torch.Size([8, 32, 2048, 108]), k.shape = torch.Size([8, 32, 2048, 108])\n",
      "rotary_mask.numel() = 3456\n",
      "tensor([[[[62, 49, 43,  ..., 66, 63, 67]],\n",
      "\n",
      "         [[53, 39, 24,  ..., 67, 59, 55]],\n",
      "\n",
      "         [[63, 50, 57,  ..., 67, 64, 66]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[59, 62, 50,  ..., 63, 70, 61]],\n",
      "\n",
      "         [[61, 51, 58,  ..., 54, 67, 66]],\n",
      "\n",
      "         [[52, 30, 24,  ..., 59, 54, 67]]]], device='cuda:0')\n",
      "seq_len = 2048, n_heads = 32, head_dims = 108\n",
      "original: rotary_mask.shape = torch.Size([1, 32, 1, 108])\n",
      "original: cos.shape = torch.Size([1, 2048, 128])\n",
      "original: sin.shape = torch.Size([1, 2048, 128])\n",
      "transformed: rotary_mask.shape = torch.Size([1, 32, 2048, 108])\n",
      "transformed: cos.shape = torch.Size([1, 32, 2048, 128])\n",
      "transformed: sin.shape = torch.Size([1, 32, 2048, 128])\n",
      "post-gather: cos.shape = torch.Size([1, 32, 2048, 108])\n",
      "post-gather: sin.shape = torch.Size([1, 32, 2048, 108])\n",
      "q_embed.shape = torch.Size([8, 32, 2048, 108]), k_embed.shape = torch.Size([8, 32, 2048, 108])\n",
      "16384\n"
     ]
    }
   ],
   "source": [
    "model_compressed.cuda()\n",
    "compressed_ppl = compute_perplexity(\n",
    "    model_compressed, tokenizer_compressed, eval_texts, device=args.device\n",
    ")\n",
    "logger.info(f\"Compressed model perplexity on WikiText2: {compressed_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ccfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 19:45:47,425 - INFO - Compressed model perplexity on WikiText2: 3620.45\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Compressed model perplexity on WikiText2: {compressed_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b8564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4ba3c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 22, 38, 54])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fc4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/sgao1/cc22bc.fsu/prog/MoDeGPT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_path = /blue/sgao1/cc22bc.fsu/prog/MoDeGPT/compressed_output/llama2-7b/rotary_masks.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from model_utils import reload_compressed_model\n",
    "\n",
    "import torch\n",
    "from eval import compute_perplexity, load_calibration_texts, load_eval_texts\n",
    "\n",
    "model, tokenizer = reload_compressed_model(\"./compressed_output/llama2-7b\", device=0)\n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "eval_texts = load_eval_texts(\n",
    "    2, model, tokenizer, batch_size=2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03553515",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_ppl = compute_perplexity(model, tokenizer, eval_texts, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compression_utils import get_Q_K_weights, get_V_O_weights\n",
    "for i in range(32):\n",
    "    W_q, W_k = get_Q_K_weights(model, i)\n",
    "    W_v, W_o = get_V_O_weights(model, i)\n",
    "\n",
    "    if (~torch.isfinite(W_q)).any():\n",
    "        print(f\"W_q has Nan/Inf\")\n",
    "\n",
    "    # Check W_k\n",
    "    if (~torch.isfinite(W_k)).any():\n",
    "        print(f\"W_k has Nan/Inf\")\n",
    "\n",
    "    # Check W_v\n",
    "    if (~torch.isfinite(W_v)).any():\n",
    "        print(f\"W_v has Nan/Inf\")\n",
    "\n",
    "    # Check W_o\n",
    "    if (~torch.isfinite(W_o)).any():\n",
    "        print(f\"W_o has Nan/Inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603cefb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m a = torch.tensor([\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m], dtype=torch.bool)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ~a:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhi\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([True, False], dtype=torch.bool)\n",
    "if not a:\n",
    "    print(f\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df388a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
